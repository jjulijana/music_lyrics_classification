{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2efd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxlen_optimizer import MaxLengthOptimizer\n",
    "from splitter import DataSplitter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd74a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../data/processed/preprocessed_lyrics.csv\"\n",
    "OUTPUT_DIR = \"../data/splits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25f9f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/processed/preprocessed_lyrics.csv\n",
      "Dataset loaded: 551,398\n",
      "\n",
      "Emotion distribution in full dataset:\n",
      "  joy: 209,003 samples (0.379)\n",
      "  sadness: 171,071 samples (0.310)\n",
      "  anger: 109,673 samples (0.199)\n",
      "  fear: 28,096 samples (0.051)\n",
      "  love: 27,963 samples (0.051)\n",
      "  surprise: 5,592 samples (0.010)\n",
      "\n",
      "=== Split Statistics ===\n",
      "TRAIN SET: 441,118 samples\n",
      "  joy: 167,202 (0.379)\n",
      "  sadness: 136,857 (0.310)\n",
      "  anger: 87,738 (0.199)\n",
      "  fear: 22,476 (0.051)\n",
      "  love: 22,371 (0.051)\n",
      "  surprise: 4,474 (0.010)\n",
      "VAL SET: 55,140 samples\n",
      "  joy: 20,900 (0.379)\n",
      "  sadness: 17,107 (0.310)\n",
      "  anger: 10,968 (0.199)\n",
      "  fear: 2,810 (0.051)\n",
      "  love: 2,796 (0.051)\n",
      "  surprise: 559 (0.010)\n",
      "TEST SET: 55,140 samples\n",
      "  joy: 20,901 (0.379)\n",
      "  sadness: 17,107 (0.310)\n",
      "  anger: 10,967 (0.199)\n",
      "  fear: 2,810 (0.051)\n",
      "  love: 2,796 (0.051)\n",
      "  surprise: 559 (0.010)\n",
      "Train split size: 441118\n",
      "  Class distribution: {np.int64(0): np.int64(87738), np.int64(1): np.int64(22476), np.int64(2): np.int64(167202), np.int64(3): np.int64(22371), np.int64(4): np.int64(136857), np.int64(5): np.int64(4474)}\n",
      "Val split size: 55140\n",
      "  Class distribution: {np.int64(0): np.int64(10968), np.int64(1): np.int64(2810), np.int64(2): np.int64(20900), np.int64(3): np.int64(2796), np.int64(4): np.int64(17107), np.int64(5): np.int64(559)}\n",
      "Test split size: 55140\n",
      "  Class distribution: {np.int64(0): np.int64(10967), np.int64(1): np.int64(2810), np.int64(2): np.int64(20901), np.int64(3): np.int64(2796), np.int64(4): np.int64(17107), np.int64(5): np.int64(559)}\n",
      "Total samples in splits: 551398\n",
      "Total samples in dataset: 551398\n"
     ]
    }
   ],
   "source": [
    "splitter = DataSplitter(CSV_PATH)\n",
    "splitter.load_data()\n",
    "splitter.create_splits(train_size=0.8, val_size=0.1, test_size=0.1)\n",
    "\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    texts, labels = splitter.get_split_for_pytorch(split_name)\n",
    "    print(f\"{split_name.title()} split size: {len(texts)}\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"  Class distribution: {dict(zip(unique, counts))}\")\n",
    "    splitter.compute_class_weights('inverse', use_split=split_name)\n",
    "    splitter.compute_class_weights('sqrt', use_split=split_name)\n",
    "\n",
    "total = sum(len(splitter.get_split_for_pytorch(name)[0]) for name in ['train', 'val', 'test'])\n",
    "print(f\"Total samples in splits: {total}\")\n",
    "print(f\"Total samples in dataset: {len(splitter.df)}\")\n",
    "assert total == len(splitter.df), \"Splits do not cover the whole dataset!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f156213",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.compute_all_class_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5669ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split: ../data/splits\\train.csv (441,118 samples)\n",
      "Saved val split: ../data/splits\\val.csv (55,140 samples)\n",
      "Saved test split: ../data/splits\\test.csv (55,140 samples)\n",
      "Saved label encoder: ../data/splits\\label_encoder.joblib\n",
      "Saved metadata: ../data/splits\\split_metadata.json\n"
     ]
    }
   ],
   "source": [
    "splitter.save_splits(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b42b7",
   "metadata": {},
   "source": [
    "\n",
    "| Name             | Train | Val | Test | Use Case/Recommendation                |\n",
    "|------------------|-------|-----|------|----------------------------------------|\n",
    "| Standard         | 0.8   | 0.1 | 0.1  |  Maximizing training data, enough for validation and testing. |\n",
    "| Large validation | 0.7   | 0.2 | 0.1  |  Tuning hyperparameters more thoroughly or need more validation data for early stopping. |\n",
    "| Large test       | 0.7   | 0.1 | 0.2  |  Estimating more robust model performance. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988b67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_max_length_optimization():\n",
    "    \"\"\"Test different max_length optimization strategies\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING MAX LENGTH OPTIMIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    coverage_targets = [0.85, 0.90, 0.95, 0.99]\n",
    "    \n",
    "    results = {}\n",
    "    for target in coverage_targets:\n",
    "        print(f\"\\n--- Coverage Target: {target*100:.0f}% ---\")\n",
    "        \n",
    "        optimal_length = MaxLengthOptimizer.get_optimal_max_length(\n",
    "            CSV_PATH, \n",
    "            coverage_target=target,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        results[target] = optimal_length\n",
    "    \n",
    "    print(f\"\\n=== Comparison Summary ===\")\n",
    "    for target, length in results.items():\n",
    "        memory_factor = length / min(results.values())\n",
    "        print(f\"Coverage {target*100:.0f}%: max_length={length} (memory factor: {memory_factor:.1f}x)\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99bcd372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING MAX LENGTH OPTIMIZATION\n",
      "============================================================\n",
      "\n",
      "--- Coverage Target: 85% ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Max Length Analysis ===\n",
      "Dataset: ../data/processed/preprocessed_lyrics.csv\n",
      "Samples analyzed: 10000\n",
      "Mean length: 350.9\n",
      "95th percentile: 731.0\n",
      "Target coverage: 85.0%\n",
      "Optimal max_length: 688\n",
      "Estimated truncation: ~15.0% of samples\n",
      "\n",
      "--- Coverage Target: 90% ---\n",
      "=== Max Length Analysis ===\n",
      "Dataset: ../data/processed/preprocessed_lyrics.csv\n",
      "Samples analyzed: 10000\n",
      "Mean length: 350.5\n",
      "95th percentile: 752.0\n",
      "Target coverage: 90.0%\n",
      "Optimal max_length: 704\n",
      "Estimated truncation: ~10.0% of samples\n",
      "\n",
      "--- Coverage Target: 95% ---\n",
      "=== Max Length Analysis ===\n",
      "Dataset: ../data/processed/preprocessed_lyrics.csv\n",
      "Samples analyzed: 10000\n",
      "Mean length: 350.0\n",
      "95th percentile: 751.0\n",
      "Target coverage: 95.0%\n",
      "Optimal max_length: 832\n",
      "Estimated truncation: ~5.0% of samples\n",
      "\n",
      "--- Coverage Target: 99% ---\n",
      "=== Max Length Analysis ===\n",
      "Dataset: ../data/processed/preprocessed_lyrics.csv\n",
      "Samples analyzed: 10000\n",
      "Mean length: 352.7\n",
      "95th percentile: 751.0\n",
      "Target coverage: 99.0%\n",
      "Optimal max_length: 1024\n",
      "Estimated truncation: ~1.0% of samples\n",
      "\n",
      "=== Comparison Summary ===\n",
      "Coverage 85%: max_length=688 (memory factor: 1.0x)\n",
      "Coverage 90%: max_length=704 (memory factor: 1.0x)\n",
      "Coverage 95%: max_length=832 (memory factor: 1.2x)\n",
      "Coverage 99%: max_length=1024 (memory factor: 1.5x)\n"
     ]
    }
   ],
   "source": [
    "max_length_results = test_max_length_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0deb75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_truncation_with_max_lengths(csv_path, max_length_results, tokenizer=None, sample_size=1000):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    texts = df[\"text\"].dropna().tolist()\n",
    "    texts_sample = texts[:min(sample_size, len(texts))]\n",
    "\n",
    "    if tokenizer:\n",
    "        lengths = [len(tokenizer(text, truncation=False, padding=False)['input_ids']) for text in texts_sample]\n",
    "    else:\n",
    "        lengths = [len(text.split()) for text in texts_sample]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRUNCATION ANALYSIS USING max_length_results\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for coverage, max_length in max_length_results.items():\n",
    "        samples_truncated = sum(1 for l in lengths if l > max_length)\n",
    "        truncation_rate = samples_truncated / len(lengths)\n",
    "        avg_words_lost = (\n",
    "            sum(max(0, l - max_length) for l in lengths) / samples_truncated if samples_truncated else 0\n",
    "        )\n",
    "        print(f\"\\nCoverage target: {coverage*100:.0f}%\")\n",
    "        print(f\"  Max length: {max_length}\")\n",
    "        print(f\"  Truncation rate: {truncation_rate:.1%}\")\n",
    "        print(f\"  Samples truncated: {samples_truncated:,}\")\n",
    "        print(f\"  Avg words lost per truncated sample: {avg_words_lost:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd96bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRUNCATION ANALYSIS USING max_length_results\n",
      "============================================================\n",
      "\n",
      "Coverage target: 85%\n",
      "  Max length: 688\n",
      "  Truncation rate: 6.3%\n",
      "  Samples truncated: 3,132\n",
      "  Avg words lost per truncated sample: 206.3\n",
      "\n",
      "Coverage target: 90%\n",
      "  Max length: 704\n",
      "  Truncation rate: 5.7%\n",
      "  Samples truncated: 2,847\n",
      "  Avg words lost per truncated sample: 210.1\n",
      "\n",
      "Coverage target: 95%\n",
      "  Max length: 832\n",
      "  Truncation rate: 2.4%\n",
      "  Samples truncated: 1,189\n",
      "  Avg words lost per truncated sample: 296.2\n",
      "\n",
      "Coverage target: 99%\n",
      "  Max length: 1024\n",
      "  Truncation rate: 0.8%\n",
      "  Samples truncated: 409\n",
      "  Avg words lost per truncated sample: 526.6\n"
     ]
    }
   ],
   "source": [
    "analyze_truncation_with_max_lengths(CSV_PATH, max_length_results, sample_size=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a1893",
   "metadata": {},
   "source": [
    "**Key Results for random sample size = 50000 for LSTM:**\n",
    "\n",
    "| Coverage Target | Max Length | Truncation Rate | Samples Truncated | Avg Words Lost | Memory Factor |\n",
    "|-----------------|------------|-----------------|-------------------|----------------|--------------|\n",
    "| 85%             | 688        | 6.3%            | 3,132             | 206.3          | 1.0x         |\n",
    "| 90%             | 704        | 5.7%            | 2,847             | 210.1          | 1.0x         |\n",
    "| **95%**         | **832**    | **2.4%**        | **1,189**         | **296.2**      | **1.2x**     |\n",
    "| 99%             | 1024       | 0.8%            | 409               | 526.6          | 1.5x         |\n",
    "\n",
    "**Decision:**  \n",
    "We will use **max_length = 832** for all splits and model training.  \n",
    "This setting preserves 95% of samples without truncation, keeps memory usage reasonable, and minimizes information loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
